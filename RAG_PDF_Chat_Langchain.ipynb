{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bO-5fakxt9ay"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install langchain langchain-community langchain-openai\n",
        "!pip install chromadb\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "ErtWsa41uChB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key:\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0sDi8cmHuCj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Document Processing Functions\n",
        "def load_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Load PDF and extract text\n",
        "    \"\"\"\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    print(f\"Loaded {len(documents)} pages from PDF\")\n",
        "    return documents\n",
        "\n",
        "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks for better retrieval\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(splits)} chunks\")\n",
        "    return splits"
      ],
      "metadata": {
        "id": "PqF6DhuYuCnB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Embeddings\n",
        "def setup_embeddings():\n",
        "    \"\"\"\n",
        "    Initialize embedding model - using free HuggingFace model\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "    print(\"Embeddings model loaded successfully!\")\n",
        "    return embeddings\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = setup_embeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUQwyUd4uCp4",
        "outputId": "257d9f77-c90e-443c-b51c-4d4c25ba085a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Database Functions\n",
        "def create_vector_store(documents, embeddings):\n",
        "    \"\"\"\n",
        "    Create vector database from document chunks\n",
        "    \"\"\"\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=\"./chroma_db\"  # Save to disk\n",
        "    )\n",
        "    print(f\"Created vector store with {len(documents)} documents\")\n",
        "    return vectorstore\n",
        "\n",
        "def load_existing_vector_store(embeddings):\n",
        "    \"\"\"\n",
        "    Load existing vector database if it exists\n",
        "    \"\"\"\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=\"./chroma_db\",\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "F9I-x0vquCs6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RAG Chain Setup\n",
        "def create_rag_chain(vectorstore):\n",
        "    \"\"\"\n",
        "    Create the conversational RAG chain\n",
        "    \"\"\"\n",
        "    # Create retriever from vector store\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}  # Retrieve top 3 most similar chunks\n",
        "    )\n",
        "\n",
        "    # Initialize LLM\n",
        "    llm = OpenAI(temperature=0.7)\n",
        "\n",
        "    # Create memory for conversation history\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "\n",
        "    # Create conversational chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"RAG chain created successfully!\")\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "ufv9sFTGuCvw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF Upload and Processing Function\n",
        "def process_pdf_file(uploaded_file):\n",
        "    \"\"\"\n",
        "    Process uploaded PDF file and create vector store\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Save uploaded file temporarily\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "            tmp_file.write(uploaded_file.read())\n",
        "            tmp_file_path = tmp_file.name\n",
        "\n",
        "        print(\"Step 1: Loading PDF...\")\n",
        "        documents = load_pdf(tmp_file_path)\n",
        "\n",
        "        print(\"Step 2: Splitting documents...\")\n",
        "        splits = split_documents(documents)\n",
        "\n",
        "        print(\"Step 3: Creating vector store...\")\n",
        "        vectorstore = create_vector_store(splits, embeddings)\n",
        "\n",
        "        print(\"Step 4: Creating RAG chain...\")\n",
        "        qa_chain = create_rag_chain(vectorstore)\n",
        "\n",
        "        # Clean up temporary file\n",
        "        os.unlink(tmp_file_path)\n",
        "\n",
        "        print(\"PDF processed successfully! You can now ask questions.\")\n",
        "        return qa_chain, vectorstore\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "J16aQDf1uCzE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Interface Function\n",
        "def chat_with_pdf(qa_chain, question):\n",
        "    \"\"\"\n",
        "    Ask questions about the PDF content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"Searching relevant content...\")\n",
        "\n",
        "        # Get response from RAG chain\n",
        "        response = qa_chain({\"question\": question})\n",
        "\n",
        "        print(f\"Answer: {response['answer']}\")\n",
        "\n",
        "        # Show source documents if available\n",
        "        if 'source_documents' in response and response['source_documents']:\n",
        "            print(\"\\n Sources used:\")\n",
        "            for i, doc in enumerate(response['source_documents']):\n",
        "                print(f\"   Source {i+1}: Page {doc.metadata.get('page', 'Unknown')}\")\n",
        "                print(f\"   Content preview: {doc.page_content[:200]}...\")\n",
        "                print()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during chat: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "VKm68hSPuC12"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File Upload Helper for Colab\n",
        "from google.colab import files\n",
        "\n",
        "def upload_and_process_pdf():\n",
        "    \"\"\"\n",
        "    Upload PDF file in Colab and process it\n",
        "    \"\"\"\n",
        "    print(\"Please upload your PDF file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        # Get the first uploaded file\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        file_content = uploaded[filename]\n",
        "\n",
        "        print(f\"Processing {filename}...\")\n",
        "\n",
        "        # Create a file-like object\n",
        "        import io\n",
        "        file_obj = io.BytesIO(file_content)\n",
        "\n",
        "        # Process the PDF\n",
        "        qa_chain, vectorstore = process_pdf_file(file_obj)\n",
        "\n",
        "        return qa_chain, vectorstore\n",
        "    else:\n",
        "        print(\"No file uploaded\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "hdpsMAyxuC4q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the System\n",
        "print(\"RAG PDF Chatbot System Ready!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Global variables to store our system\n",
        "qa_chain = None\n",
        "vectorstore = None\n",
        "\n",
        "print(\" System initialized. Ready to process PDF!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srI4RgV9uC7q",
        "outputId": "4390541b-3ada-4274-ea7a-9dc9e5430e33"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG PDF Chatbot System Ready!\n",
            "==================================================\n",
            " System initialized. Ready to process PDF!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload and Process PDF\n",
        "print(\"Upload and Process Your PDF\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Upload and process PDF\n",
        "qa_chain, vectorstore = upload_and_process_pdf()\n",
        "\n",
        "if qa_chain:\n",
        "    print(\"Success! Your PDF is now ready for questions.\")\n",
        "    print(\"You can now ask questions about the content in the next cell.\")\n",
        "else:\n",
        "    print(\"Failed to process PDF. Please try again.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vT47OIA-zT3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Chat Interface\n",
        "def start_chat():\n",
        "    \"\"\"\n",
        "    Start interactive chat session\n",
        "    \"\"\"\n",
        "    if qa_chain is None:\n",
        "        print(\"Please upload and process a PDF first!\")\n",
        "        return\n",
        "\n",
        "    print(\"üí¨ Chat with your PDF - Type 'quit' to exit\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\n Your question: \").strip()\n",
        "\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not question:\n",
        "            print(\"Please enter a question.\")\n",
        "            continue\n",
        "\n",
        "        # Get response\n",
        "        response = chat_with_pdf(qa_chain, question)\n",
        "\n",
        "        if response:\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "# Start the chat\n",
        "start_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XWJW3YzizT6-",
        "outputId": "09466e79-d173-4564-be6d-626c565a8a4f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload and process a PDF first!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Token limit in api call of OpenAI"
      ],
      "metadata": {
        "id": "pvhR7amI1Bzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##FreeModel- Using local model"
      ],
      "metadata": {
        "id": "1psMSOh1zT9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QG0SHU6h1C8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Free Local LLM\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "def setup_free_llm():\n",
        "    \"\"\"\n",
        "    Setup free local language model\n",
        "    \"\"\"\n",
        "    print(\" Loading free local model (this may take a few minutes first time)...\")\n",
        "\n",
        "    # Use a smaller, efficient model that works well in Colab\n",
        "    model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "    # Alternative models you can try:\n",
        "    # \"google/flan-t5-base\" - Good for Q&A\n",
        "    # \"microsoft/DialoGPT-small\" - Smaller, faster\n",
        "\n",
        "    try:\n",
        "        # Create pipeline\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            tokenizer=model_name,\n",
        "            max_length=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "        )\n",
        "\n",
        "        # Wrap in LangChain\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"Free local model loaded successfully!\")\n",
        "        return llm\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Trying alternative model...\")\n",
        "\n",
        "        # Fallback to even smaller model\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"gpt2\",\n",
        "            max_length=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"Fallback model loaded!\")\n",
        "        return llm\n",
        "\n",
        "# Load the free model\n",
        "free_llm = setup_free_llm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdfHMcdqzUAk",
        "outputId": "4c79a372-6b21-4eb4-c907-2ea9c76db21f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading free local model (this may take a few minutes first time)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free local model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Current Variables\n",
        "print(\"üîç Checking current variables...\")\n",
        "print(f\"vectorstore exists: {'vectorstore' in globals() and vectorstore is not None}\")\n",
        "print(f\"qa_chain exists: {'qa_chain' in globals() and qa_chain is not None}\")\n",
        "print(f\"embeddings exists: {'embeddings' in globals() and embeddings is not None}\")\n",
        "print(f\"free_llm exists: {'free_llm' in globals() and free_llm is not None}\")\n",
        "\n",
        "# Check if we have any Chroma database saved\n",
        "import os\n",
        "chroma_exists = os.path.exists('./chroma_db')\n",
        "print(f\"Chroma database exists on disk: {chroma_exists}\")"
      ],
      "metadata": {
        "id": "mBCvU_DfCA50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified RAG Chain with Free LLM\n",
        "def create_free_rag_chain(vectorstore):\n",
        "    \"\"\"\n",
        "    Create RAG chain using free local model\n",
        "    \"\"\"\n",
        "    # Create retriever\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 2}  # Reduced to 2 for smaller context\n",
        "    )\n",
        "\n",
        "    # Use our free LLM\n",
        "    llm = free_llm\n",
        "\n",
        "    # Create memory\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "\n",
        "    # Create chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Free RAG chain created!\")\n",
        "    return qa_chain\n",
        "\n",
        "# If you already have a vectorstore from before, recreate the chain\n",
        "if vectorstore is not None:\n",
        "    print(\"üîÑ Recreating RAG chain with free model...\")\n",
        "    qa_chain = create_free_rag_chain(vectorstore)\n",
        "    print(\"‚úÖ Ready to chat with free model!\")\n",
        "else:\n",
        "    print(\"üì§ Please upload a PDF first using the upload function\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOfuGgUH0nKa",
        "outputId": "603303a8-477e-480e-e5f7-f003397d7ee1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Please upload a PDF first using the upload function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Created Simpler QA Function\n",
        "def simple_qa(question):\n",
        "    \"\"\"\n",
        "    Simple QA function that works with free model\n",
        "    \"\"\"\n",
        "    print(f\"ü§î Question: {question}\")\n",
        "\n",
        "    # Get relevant chunks\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    if not docs:\n",
        "        print(\"‚ùå No relevant documents found\")\n",
        "        return\n",
        "\n",
        "    # Combine context\n",
        "    context = \"\\n\".join([doc.page_content[:300] for doc in docs])\n",
        "\n",
        "    # Create simple prompt\n",
        "    prompt = f\"\"\"Based on the following context, answer the question:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    print(\"ü§ñ Generating answer...\")\n",
        "    try:\n",
        "        answer = free_llm.predict(prompt)\n",
        "        print(f\"‚úÖ Answer: {answer}\")\n",
        "\n",
        "        # Show sources\n",
        "        print(\"\\nüìÑ Sources:\")\n",
        "        for i, doc in enumerate(docs):\n",
        "            print(f\"Source {i+1}: {doc.page_content[:100]}...\")\n",
        "\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Test the simple function\n",
        "simple_qa(\"What is this document about?\")"
      ],
      "metadata": {
        "id": "Dk7inOR300Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Chat with Simple Function\n",
        "def simple_chat():\n",
        "    \"\"\"\n",
        "    Simple chat interface using our working function\n",
        "    \"\"\"\n",
        "    print(\"üí¨ Simple PDF Chat (type 'quit' to exit)\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nü§î Your question: \").strip()\n",
        "\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        simple_qa(question)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Start simple chat\n",
        "simple_chat()"
      ],
      "metadata": {
        "id": "UQWhGliO00IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRQdrYueuC-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}